\documentclass{standalone}

\begin{document}
We test each pivoting strategy on a select subset of the Netlib dataset \cite{netlibtest}. The reason for not testing on the whole dataset is that the programs implemented in appendices \ref{appendix:test} through \ref{appendix:clarkson} are not efficient enough to solve most of the problems in a reasonable amount of time. We observe that we can solve problems that are of size $<1000$ constraints and $<1000$ variables so we select all such problems from the dataset. The function given in appendix \ref{appendix:test} kicks off the evaluation process and for each test file runs all of the pivoting methods. We store the optimal objective value found, the number of iterations a method took, the CPU time taken and an exit flag. If the exit flag is $-1$ then the method reached the maximum number of iterations, if the exit flag is $0$ then the method found an optimal vertex, if the exit flag is $1$ then the problem is infeasible and if the exit flag is $2$ then the problem is unbounded. Since the problems we consider are all feasible and bounded we should never observe the exit flags of $1$ and $2$ \cite{netlibtest}. The test function also checks for singular matrix warnings given by Matlab and records whether such a warning was given. If a warning is detected then that run of a pivoting strategy is marked with a $1$ and otherwise it is $0$. In the case of a warning, the exit code could be $1$ and $2$ due to numeriacal instability caused by using singular matrices. The reason for why this is done is discussed in section \ref{sec:eval}. We also enclose everything in a try-catch block so if Matlab fails due to either possible ill-conditioning of a problem or a specific method's inability to handle the problem's size, we can continue processing the next dataset and ignore the problematic dataset. However, we have no way of recovering the problem message for now.\par
The next testing function is the one in appendix \ref{appendix:test} that performs a $2$ phase LP. It begins by reading from a MPS file that defines a problem in the dataset and then starts phase $1$. We have found that the increase in constraints and variables caused by setting up a phase $1$ LP makes our pivoting methods perform especially slow so we let Matlab's internal \verb|linprog| solver find an initial feasible vertex. We do, however, actually set up the problem before passing it onto \verb|linprog|. Then a phase $2$ LP is done which calls a function depending on the \verb|option| parameter where each function employs a specific pivoting strategy. There is also some boilerplate code that records the number of iterations, the exit flags and the CPU time of the calls using Matlab's \verb|tic| and \verb|toc|. The reason for not using \verb|timeit| is because \verb|timeit| runs a function $10$ times which ends up slowing down our testing process by a lot. So rather than testing the function multiple times we only take the CPU time of one run as its runtime. This can be somewhat problematic for evaluating the efficiency of randomized algorithms but we leave the discussion of the limitations of the experiment to section \ref{sec:eval}.\par
The programs given in appendices \ref{appendix:simplex}, \ref{appendix:randfacet} and \ref{appendix:clarkson} are implementations of the simplex method, the random facet method and Clarkson's first algorithm, respectively. Explanations for the theory behind each strategy is given in section \ref{sec:pivot} and the implementations stick to the theoretical presentation. The only extra piece of code that might stick out is the use of boiler plate to record statistics such as iterations and exit flags. For the random facet algorithm the number of iterations is measured as the number of pivot steps made rather than the number of recursive calls. The reason for this is that the stack size of the random facet is always bounded by the size of the working set of a linear program so counting the number of recursive calls can be misleading. Instead, since we know that the pivoting step is always performed to check for optimality we count the number of pivot steps. For Clarkson's algorithm the number of iterations are given by the sum of the number of iterations the algorithm calls everytime it reaches a base case. Finally, for the classical simplex method we say that the number of iterations taken are equivalent to the number of times the algorithm stays in the while loop. If we ever reach the maximum number of iterations then we make sure to end the execution and keep whatever result was last computed.\par
Although the implementations are considered inefficient for solving larger problems, they do still take advantage of sparsity and use sparse representations of the problem matrices. We rely on Matlab's internals to pick the best algorithm for a given context. Furthermore, a utiliy function is given in appendix \ref{appendix:rank} that estimates the rank of a matrix using the QR decomposition and is used throughout the other programs. It can be called with a specific tolerance level or it will use the default tolerance of \verb|1e-10|. Additionally, our simplex and simplex-like methods use the default tolerance and maximum iteration values Matlab's \verb|linprog| solver uses \cite{linprog}. The default constraint tolerance for the dual-simplex implementation in \verb|linprog| is \verb|1e-7| and the default maximum number of iterations is given by
\[
  \verb|10*(numberOfEqualities + numberOfInequalities + numberOfVariables)|.
\]
In our case we use the default tolerance and simplify the maximum number of iterations to
\[
  \verb|10*(numberOfInequalities + numberOfVariables)|
\]
as we have no equalitiy constraints. All implementations and files can be found in the following GitHub repository: \url{https://github.com/goki0607/randomized-pivoting-project}.\par
Finally, we run all test's on NYU Courant's Snappy $1$ server. This system is equipped with a Two Intel Xeon E5-2680 ($2.80$ GHz) ($20$ cores) CPUs, has $128$ GB of memory and runs CentOs $7$. The author's MacBook Air proved to be inadequate for testing so we use a machine whose purpose is for numerical computation.
\end{document}